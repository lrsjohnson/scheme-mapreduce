\documentclass{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{enumitem}

\usepackage{titling}

%%%%%
% Setup
%%%%%
\newcommand{\p}{\mathbb{P}}
\newcommand{\e}{\mathbb{E}}

% Margins
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

%\setlength{\droptitle}{-0.5in}

\def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}
\newcommand{\qedbox}
{\vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}}
%\newcommand{\qed}               {\nopagebreak\mbox{}\hfill\qedbox\smallskip}

\title{MapReduce System (MRS)\\
{\large 6.945 Final Project - Spring 2014}}
\author{Lars Johnson (larsj@mit.edu) \\ Gurtej Kanwar (gurtej@mit.edu)}
\date{May 12, 2014}

%%%%%
% Document
%%%%%
\begin{document}
\pagestyle{myheadings}
\maketitle

\markright{MapReduce System - 6.945 Final Project - Lars Johnson}

\section{Overview}

We looked into the problem of building a system to work with data sets
as a fundamental type. This problem was inspired by the Map Reduce algorithm:
a system which performs a distributed map operation on a data set, followed
by a reduce. We took this problem and generalized to stateful multi-maps on data
sets. The ultimate goal was to build a system which allowed the user to
use MIT Scheme to assemble a network of data-set operations, feed data set
inputs at various points in the network, and extract processed output from
the network. We successfully built a system which provided user-facing functions
to build the graph and set inputs and outputs. The user can do this both interactively
through a custom REPL environment and by defining a thunk to perform construction
and input which is executed in the default REPL by our run-computation function.

\subsection{Basic Example}

See Figure 1 for a basic example of using our system to perform the
map-reduce operation of determining word frequencies over a set of documents.
\begin{figure}[h!]
  \includegraphics[width=\textwidth]{basic-example.png}
  \caption{\emph{\small A basic example of using our system to
      performe the map-reduce operation of counting the number of
      words present in a set of documents}}
\end{figure}

\section{Context}

\subsection{Definitions}

\textbf{Data set:} A multi-set of (key,value) pairs. Because this is a multi-set, multiple
copies of the same key can exist.

\textbf{Multi-map operation:} A function from a single (key,value) pair to zero or more
(newkey, newvalue) pairs. As an example, both the map and filter operations are subsets
of this abstraction.

\textbf{Stateful multi-map operation:} A function from a single (key,value) pair and previous
state to zero or more (newkey, newvalue) pairs and new state. This is a slightly broader
abstraction than multi-map, which allowed us to cover all operations which we were interested
in implementing.


\subsection{MapReduce Background}

The inspiration for this project came from the MapReduce algorithm developed by
Jeffrey Dean and Sanjay Ghemawat at Google in 2004. The algorithm operates on a
single input data set, by first performing a distributed \textbf{map} over all of
the input (key,value) pairs, then aggregating values by keys and performing a
distributed \textbf{reduce} over each
of the lists of aggregated values.

A canonical example of a MapReduce operation is
word counts from a list of documents. To construct this MapReduce we define the
map to act on (id, document) pairs and output (word, count) pairs, and we define
the reduce to receive the aggregated word count pairs (word, [count1, count2, ...])
and output a single (word, count) pair. The implementation of map in this example
iterates over the words of the document and emits a (word, 1) pair for each word
it sees. After aggregation, the reduce operation simple has to iterate through the
given counts and sum them to output the final counts.

The advantage of framing a computation on a data set as a MapReduce operation
is that the definitions of map and reduce can completely ignore the massively
distributed nature of the actual computation and focus on the actual operations
being applied to the data. As demonstrated by Google, this has been quite
successful for performing large-scale calculations that are farmed out to thousands
of machines.


\subsection{Extending MapReduce}

While MapReduce was an interesting problem, it is a very specific method of
operating on data sets. Our goal from this project was to extend this paradigm
to a more general system of computation on data sets as fundamental units of
data. We looked at the MapReduce problem and came up with a generalization
of the operations involved to \emph{stateful multi-map} operations.

Both map and reduce are actually non-stateful multi-map functions: map accepts a (key,value) pair
and emits zero or more (newkey, newvalue) pairs in a (potentially) different
domain; reduce accepts a (key,valuelist) pair and generally emits exactly
one (newkey,newvalue) pair as output. The hidden third operation, the aggregation
step, is trickier to generalize. The aggregation step requires information
from all of its inputs at once to generate an output. This does not fit the 
mould of a simple multi-map function, since each (key,value) pair by itself
does not provide enough information to aggregate. We thus decided to broaden our
generalization to stateful multi-map operations. By allowing state, map, reduce
and aggregation could all be thought of as specific types of stateful multi-map
functions. There are several other common data-set operations that fall into this category
as well: filter a data set or combining two data sets, for example.


\section{Our System}

\subsection{Key Ideas}

Build a graph of data sets connected by operations

Feed data into data sets and it will be processed in a distributed manner across a worker pool

Abstraction system to allow for streaming implementations

Provide programmers with a combinator-like family of reusable parts

\subsection{System Architecture}

\begin{figure}[h!]
\includegraphics[width=\textwidth]{system-arch.png}
  \caption{\emph{\small The high-level architecture of our four main
      components (User Operations, Masters, Distributors, Workers).}}
\label{piefig}
\end{figure}

Our MapReduce System is comprised of four main components (see diagram
1). On the top level, a set of user operations (mrs:create-data-set)
(mrs:map...), (mrs:reduce...), (mrs:feed-data ...) etc. enable a user
of the system to specify a dataflow network and feed data into
it. Executing such specifications from within a
(mrs:run-computation...) call directs a master process to allocate the
threads and spawn a distriobutor task for each requested
computation. These distributors coordinate execution of the function
and will spawn a number of worker threads to perform the exeuction.

\subsection{Communication Details}

A key component of developing such a system is managing communications
betweent various tasks. We used conspire:threads to provide
multi-tasking behavior for the tasks and used both non-blocking pipes
and data-sets to coordinate between systems. 

The non-blocking pipes mirror the standard pipes used conspire:threads
and consist of alocked queue. They are used as our demo means
for communiczating between small groups over long ranges.

Our main ``Data Set'' objects were implemented as multi-reader
multi-writer queues of elements. These data sets were abstracted
behind a generic get-reader / get-writer which allowed us to quickly
explore a variety of useful implementations (see below).

\subsection{Flexible Implementations}

Many of these components were designed with a focus on flexibility to
admit several possible different implementations.

1) {\bf Pipes} For the inter-process communication, we pass data between our
distributors and workers via simple queued pipes. However, our data
abstraction of ``send message to worker'' and ``Get message from
worker'' could easily be reimplemented to use network / TCP
communications to communicate with an actual distributed system of computers.

2) {\bf Data sets} For intra-process communication, we developed a handful of
different data-sets that could be used for passing information from
one operation to another. Our data sets primarily consisted of
limited-sized queues that transferred partially-processed data from
to-computation. We provided three types of data-sets that users can
construct to perform diferent purposes. All three of these are
accessed via a friendly officer presence:

\subsubsection{Multi-Reader-Queue Data Set}
The main class of Data Set used in the system is based on a
multi-reader queue we built. This queue is aware of how many readers
it has and manages pointers representing each reader's location. Data
set elements remain in the queue until all readers have read the
element. 

\subsubsection{Output Data Set}
The user can specify a (single) output data set in the computation
network. This data set is special in that it passes the data it
receies back to the original caller of (mrs:run-computation...). If
the user specifies a callback function to run-computation, data set
results will be passed as (k v) arguments to the callback. Otherwise,
the output data set will enable (mrs:run-computation...) to block on
the computation until the data set is complete and is returned to the user.

\subsubsection{File Writer Data Set}
The file-writer data set mirrors that of the output data set in that
it provides a means of returning data set results to the user. Upon
creation, users specify a filepath and the data sets are incrementally
written in (k v) format to the file.

\subsection{Sink - /dev/null Data Set}

Finally, we implemented a simplistic ``sink'' data set that
corresponds to the standard ``black hole'' /dev/null file used in
Linux-based systems.

\subsection{Combinatpor System}

Finally, we are excited to be able to provide a combinator-like system
for defining and building distributed systems computation graphcs. The
syntax for defining a network closely resembles that from the
propagator system in which a user allocates a number of cells
representing data-sets, followed by specifying operations that
transform the contents of cell data sets into another another.

\section{Conceptual Challenges}

In addition to the technical difficulties of exploring the
multitasking and distributed systems paradigm in more depth, our
project involved a handful of conceptual challenges, particularly
in dealing with the propagation of ``done'' signals to properly handle
reductions and aggregations on data streams.

\subsection{Done Propagation}

...

\subsection{Detecting Completion}

... Return to the user? Depends-on

\section{Future Work}

...??

\end{document}
