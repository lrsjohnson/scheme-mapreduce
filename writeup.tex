\documentclass{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{enumitem}

\usepackage{titling}

%%%%%
% Setup
%%%%%
\newcommand{\p}{\mathbb{P}}
\newcommand{\e}{\mathbb{E}}

% Margins
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

%\setlength{\droptitle}{-0.5in}

\def\squarebox#1{\hbox to #1{\hfill\vbox to #1{\vfill}}}
\newcommand{\qedbox}
{\vbox{\hrule\hbox{\vrule\squarebox{.667em}\vrule}\hrule}}
%\newcommand{\qed}               {\nopagebreak\mbox{}\hfill\qedbox\smallskip}

\title{MapReduce System (MRS)\\
{\large 6.945 Final Project - Spring 2014}}
\author{Lars Johnson (larsj@mit.edu) \\ Gurtej Kanwar (gurtej@mit.edu)}
\date{May 12, 2014}

%%%%%
% Document
%%%%%
\begin{document}
\pagestyle{myheadings}
\maketitle

\markright{MapReduce System - 6.945 Final Project - Lars Johnson}

\section{Overview}

We looked into the problem of building a system to work with data sets
as a fundamental type. This problem was inspired by the Map Reduce algorithm:
a system which performs a distributed map operation on a data set, followed
by a reduce. We took this problem and generalized to stateful multi-maps on data
sets. The ultimate goal was to build a system which allowed the user to
use MIT Scheme to assemble a network of data-set operations, feed data set
inputs at various points in the network, and extract processed output from
the network. We successfully built a system which provided user-facing functions
to build the graph and set inputs and outputs. The user can do this both interactively
through a custom REPL environment and by defining a thunk to perform construction
and input which is executed in the default REPL by our run-computation function.

\section{Context}

\subsection{Definitions}

\textbf{Data set:} A multi-set of (key,value) pairs. Because this is a multi-set, multiple
copies of the same key can exist.

\textbf{Multi-map operation:} A function from a single (key,value) pair to zero or more
(newkey, newvalue) pairs. As an example, both the map and filter operations are subsets
of this abstraction.

\textbf{Stateful multi-map operation:} A function from a single (key,value) pair and previous
state to zero or more (newkey, newvalue) pairs and new state. This is a slightly broader
abstraction than multi-map, which allowed us to cover all operations which we were interested
in implementing.


\subsection{MapReduce Background}

The inspiration for this project came from the MapReduce algorithm developed by
Jeffrey Dean and Sanjay Ghemawat at Google in 2004. The algorithm operates on a
single input data set, by first performing a distributed \textbf{map} over all of
the input (key,value) pairs, then aggregating values by keys and performing a
distributed \textbf{reduce} over each
of the lists of aggregated values.

A canonical example of a MapReduce operation is
word counts from a list of documents. To construct this MapReduce we define the
map to act on (id, document) pairs and output (word, count) pairs, and we define
the reduce to receive the aggregated word count pairs (word, [count1, count2, ...])
and output a single (word, count) pair. The implementation of map in this example
iterates over the words of the document and emits a (word, 1) pair for each word
it sees. After aggregation, the reduce operation simple has to iterate through the
given counts and sum them to output the final counts.

The advantage of framing a computation on a data set as a MapReduce operation
is that the definitions of map and reduce can completely ignore the massively
distributed nature of the actual computation and focus on the actual operations
being applied to the data. As demonstrated by Google, this has been quite
successful for performing large-scale calculations that are farmed out to thousands
of machines.


\subsection{Extending MapReduce}

While MapReduce was an interesting problem, it is a very specific method of
operating on data sets. Our goal from this project was to extend this paradigm
to a more general system of computation on data sets as fundamental units of
data. We looked at the MapReduce problem and came up with a generalization
of the operations involved to \emph{stateful multi-map} operations.

Both map and reduce are actually non-stateful multi-map functions: map accepts a (key,value) pair
and emits zero or more (newkey, newvalue) pairs in a (potentially) different
domain; reduce accepts a (key,valuelist) pair and generally emits exactly
one (newkey,newvalue) pair as output. The hidden third operation, the aggregation
step, is trickier to generalize. The aggregation step requires information
from all of its inputs at once to generate an output. This does not fit the 
mould of a simple multi-map function, since each (key,value) pair by itself
does not provide enough information to aggregate. We thus decided to broaden our
generalization to stateful multi-map operations. By allowing state, map, reduce
and aggregation could all be thought of as specific types of stateful multi-map
functions. There are several other common data-set operations that fall into this category
as well: filter a data set or combining two data sets, for example.


\section{Our System}

\subsection{Key Ideas}

Build a graph of data sets connected by operations

Feed data into data sets and it will be processed in a distributed manner across a worker pool

Abstraction system to allow for streaming implementations

Provide programmers with a combinator-like family of reusable parts

\subsection{System Architecture}

Our MapReduce System is comprised of four main components (see diagram
1). On the top level, a set of user operations (mrs:create-data-set)
(mrs:map...), (mrs:reduce...), (mrs:feed-data ...) etc. enable a user
of the system to specify a dataflow network and feed data into
it. Executing such specifications from within a
(mrs:run-computation...) call directs a master process to allocate the
threads and spawn a distriobutor task for each requested
computation. These distributors coordinate execution of the function
and will spawn a number of worker threads to perform the exeuction.

\subsection{Communication Details}

A key component of developing such a system is managing communications
betweent various tasks. We used conspire:threads to provide
multi-tasking behavior for the tasks and used both non-blocking pipes
and data-sets to coordinate between systems. 

The non-blocking pipes mirror the standard pipes used conspire:threads
and consist of alocked queue. They are used as our demo means
for communiczating between small groups over long ranges.

Our main ``Data Set'' objects were implemented as multi-reader
multi-writer queues of elements.

Each of these data set elements were created as a 

\subsection{Flexible Implementations}

Many of these components were designed with a focus on flexibility to
admit several possible different implementations.

1) For the inter=process communication, our pipe structure

2) For intra-process communication, we developed a handful of different

\subsection{Combinator System}

Finally, we are excited to be able to provide a combinator-like system
for defining and building distributed systems computation graphcs. The
syntax for defining a network closely resembles that from the
propagator system in which a user allocates a number of cells
representing data-sets, followed by specifying operations that
transform the contents of cell data sets into another another.

\section{Conceptual Challenges}

In addition to the technical difficulties of exploring the
multitasking and distributed systems paradigm in more depth, our
project involved a handful of conceptual challenges, particularly
in dealing with the propagation of ``done'' signals to properly handle
reductions and aggregations on data streams.

\subsection{Done Propagation}

...

\subsection{Detecting Completion}

... Return to the user? Depends-on

\section{Future Work}

...??

\end{document}
